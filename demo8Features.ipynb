{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "import csv\n",
    "\n",
    "dataFile = ['acc_data', 'adl_data', 'fall_data', 'two_classes_data']\n",
    "labelFile = ['acc_labels', 'adl_labels', 'fall_labels', 'two_classes_labels']\n",
    "nameFile = ['acc_names', 'adl_names', 'fall_names', 'two_classes_names']\n",
    "\n",
    "def performance(y_true, y_pred, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Calculates the performance metric based on the agreement between the \n",
    "    true labels and the predicted labels.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        y_true -- numpy array of shape (n,), known labels\n",
    "        y_pred -- numpy array of shape (n,), (continuous-valued) predictions\n",
    "        metric -- string, option used to select the performance measure\n",
    "                  options: 'accuracy', 'f1-score', 'auroc', 'precision',\n",
    "                           'sensitivity', 'specificity'        \n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        score  -- float, performance score\n",
    "    \"\"\"\n",
    "    # map continuous-valued predictions to binary labels\n",
    "    y_label = np.sign(y_pred)\n",
    "    y_label[y_label==0] = 1\n",
    "    \n",
    "    m = metrics.confusion_matrix(y_true, y_label)\n",
    "    if (metric == \"accuracy\"): return metrics.accuracy_score(y_true, y_label)\n",
    "    if (metric == \"f1_score\"): return metrics.f1_score(y_true, y_label) \n",
    "    if (metric == \"auroc\"): return metrics.roc_auc_score(y_true, y_label)\n",
    "    if (metric == \"precision\"): return metrics.precision_score(y_true, y_label)   \n",
    "    if (metric == \"sensitivity\"): \n",
    "        TP = m[0, 0]\n",
    "        FN = m[0, 1]\n",
    "        if (TP + FN == 0.0):\n",
    "            return 0.0\n",
    "        else:\n",
    "            return float(TP) / float(TP + FN)\n",
    "    if (metric == \"specificity\"): \n",
    "        TN = m[1, 1]\n",
    "        FP = m[0, 1]\n",
    "        if (FP + TN == 0.0):\n",
    "            return 0.0\n",
    "        else:\n",
    "            return float(TN) / float(FP + TN)\n",
    "    return 0\n",
    "\n",
    "def cv_performance(clf, X, y, kf, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Splits the data, X and y, into k-folds and runs k-fold cross-validation.\n",
    "    Trains classifier on k-1 folds and tests on the remaining fold.\n",
    "    Calculates the k-fold cross-validation performance metric for classifier\n",
    "    by averaging the performance across folds.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        clf    -- classifier (instance of SVC)\n",
    "        X      -- numpy array of shape (n,d), feature vectors\n",
    "                    n = number of examples\n",
    "                    d = number of features\n",
    "        y      -- numpy array of shape (n,), binary labels {1,-1}\n",
    "        kf     -- cross_validation.KFold or cross_validation.StratifiedKFold\n",
    "        metric -- string, option used to select performance measure\n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        score   -- float, average cross-validation performance across k folds\n",
    "    \"\"\"\n",
    "    ave_performance = 0\n",
    "    num = 0\n",
    "    for train_index, test_index in kf:\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        clf.fit(X_train, y_train)\n",
    "        ave_performance += performance(y_test, clf.decision_function(X_test), metric)\n",
    "        num += 1\n",
    "    return ave_performance / num\n",
    "\n",
    "def select_param_linear(X, y, kf, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Sweeps different settings for the hyperparameter of a linear-kernel SVM,\n",
    "    calculating the k-fold CV performance for each setting, then selecting the\n",
    "    hyperparameter that 'maximize' the average k-fold CV performance.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        X      -- numpy array of shape (n,d), feature vectors\n",
    "                    n = number of examples\n",
    "                    d = number of features\n",
    "        y      -- numpy array of shape (n,), binary labels {1,-1}\n",
    "        kf     -- cross_validation.KFold or cross_validation.StratifiedKFold\n",
    "        metric -- string, option used to select performance measure\n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        C -- float, optimal parameter value for linear-kernel SVM\n",
    "    \"\"\"\n",
    "    \n",
    "    print ('Linear SVM Hyperparameter Selection based on ' + str(metric) + ':')\n",
    "    C_range = 10.0 ** np.arange(-3, 3)\n",
    "    \n",
    "    for c in C_range:\n",
    "        clf = SVC(kernel=\"linear\", C=c)\n",
    "        performance = cv_performance(clf, X, y, kf, metric)\n",
    "        if (performance > p_max):\n",
    "            p_max = performance\n",
    "            c_return = c\n",
    "        performance_list.append(performance)\n",
    "    print (performance_list)\n",
    "    return c_return\n",
    "def select_param_rbf(X, y, kf, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Sweeps different settings for the hyperparameters of an RBF-kernel SVM,\n",
    "    calculating the k-fold CV performance for each setting, then selecting the\n",
    "    hyperparameters that 'maximize' the average k-fold CV performance.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        X       -- numpy array of shape (n,d), feature vectors\n",
    "                     n = number of examples\n",
    "                     d = number of features\n",
    "        y       -- numpy array of shape (n,), binary labels {1,-1}\n",
    "        kf     -- cross_validation.KFold or cross_validation.StratifiedKFold\n",
    "        metric  -- string, option used to select performance measure\n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        gamma, C -- tuple of floats, optimal parameter values for an RBF-kernel SVM\n",
    "    \"\"\"\n",
    "    \n",
    "    print ('RBF SVM Hyperparameter Selection based on ' + str(metric) + ':')\n",
    "    C_range = 10.0 ** np.arange(-3, 3)\n",
    "    gamma_range = 10.0 ** np.arange(-4, 2)\n",
    "\n",
    "    param_grid = []\n",
    "    first_row = ['C/gamma']\n",
    "    for gamma in gamma_range:\n",
    "        first_row.append(gamma)\n",
    "    param_grid.append(first_row)\n",
    "\n",
    "    p_max = -1\n",
    "    for c in C_range:\n",
    "        row_c = []\n",
    "        row_c.append(c)\n",
    "        for gamma in gamma_range:\n",
    "            clf = SVC(kernel=\"rbf\", C=c, gamma=gamma)\n",
    "            performance = cv_performance(clf, X, y, kf, metric)\n",
    "            row_c.append(performance)\n",
    "            if (performance > p_max):\n",
    "                p_max = performance\n",
    "                c_return = c\n",
    "                g_return = gamma\n",
    "            #performance_list.append(performance)\n",
    "            print (\"C = \", c, \" Gamma=\", gamma, \" performance=\", performance) \n",
    "        param_grid.append(row_c)\n",
    "\n",
    "    \"\"\"Write out predictions to csv file.\"\"\"\n",
    "    out = open(\"param_rbf_\" + str(metric) + \".csv\", 'wb')\n",
    "    f = csv.writer(out)\n",
    "    f.writerows(param_grid)\n",
    "    out.close()\n",
    "\n",
    "    return c_return, g_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance for SVC\n",
      "accuracy = 0.937688434541\n",
      "f1_score = 0.865624074048\n",
      "auroc = 0.90062625129\n",
      "precision = 0.908641134572\n",
      "sensitivity = 0.9725261447808764\n",
      "specificity = 0.9086411345720414\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    temp_data = sio.loadmat('./demoFeaturesOutput/feat.mat')\n",
    "    X = temp_data['feat']\n",
    "    temp_label = sio.loadmat('./demoFeaturesOutput/labels.mat')\n",
    "    y = temp_label['labels'][0]\n",
    "    #data = load_data()\n",
    "    #X = data.X\n",
    "    #y = data.y\n",
    "    \n",
    "    \n",
    "    metric_list = [\"accuracy\", \"f1_score\", \"auroc\", \"precision\", \"sensitivity\", \"specificity\"]\n",
    "    \n",
    "    # for each metric, select optimal hyperparameter for linear-kernel SVM using CV\n",
    "    \n",
    "    #skf = StratifiedKFold(y, n_folds=5)\n",
    "    #for metric in metric_list:\n",
    "    #    param_linear = select_param_linear(X, y, skf, metric)\n",
    "    #    print(\"param_linear based on \", metric, \"=\", param_linear)\n",
    "\n",
    "\n",
    "    # for each metric, select optimal hyperparameter for linear-kernel SVM using CV\n",
    "    #skf = StratifiedKFold(y, n_folds=5)\n",
    "    #for metric in metric_list:\n",
    "    #    param_rbf = select_param_rbf(X, y, skf, metric)\n",
    "    #    print(\"param_rbf based on \", metric, \"=\", param_rbf)\n",
    "\n",
    "\n",
    "    skf = StratifiedKFold(y, n_folds=5)\n",
    "    clf_linear = SVC(kernel=\"linear\", C=0.001)\n",
    "    print (\"Performance for SVC\")\n",
    "    for metric in metric_list:\n",
    "        ave_performance = cv_performance(clf_linear, X, y, skf, metric=metric)\n",
    "        print(metric, \"=\", ave_performance)\n",
    "if __name__ == \"__main__\" :\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
